@inproceedings{backEvolutionaryComputationOverview1996,
  title = {Evolutionary Computation: {{An}} Overview},
  shorttitle = {Evolutionary Computation},
  booktitle = {Proceedings of {{IEEE International Conference}} on {{Evolutionary Computation}}},
  author = {Back, T. and Schwefel, H.-P.},
  year = {1996},
  month = may,
  pages = {20--29},
  doi = {10.1109/ICEC.1996.542329},
  abstract = {We present an overview of the most important representatives of algorithms gleaned from natural evolution, so-called evolutionary algorithms. Evolution strategies, evolutionary programming, and genetic algorithms are summarized, with special emphasis on the principle of strategy parameter self-adaptation utilized by the first two algorithms to learn their own strategy parameters such as mutation variances and covariances. Some experimental results are presented which demonstrate the working principle and robustness of the self-adaptation methods used in evolution strategies and evolutionary programming. General principles of evolutionary algorithms are discussed, and we identify certain properties of natural evolution which might help to improve the problem solving capabilities of evolutionary algorithms even further.},
  keywords = {Algorithm design and analysis,Europe,Evolution (biology),Evolutionary computation,Genetic algorithms,Genetic mutations,Genetic programming,Optimization methods,Problem-solving,Robustness},
  note = {105 citations (Crossref) [2023-05-31]}
}

@incollection{eshelmanCHCAdaptiveSearch1991,
  title = {The {{CHC Adaptive Search Algorithm}}: {{How}} to {{Have Safe Search When Engaging}} in {{Nontraditional Genetic Recombination}}},
  shorttitle = {The {{CHC Adaptive Search Algorithm}}},
  booktitle = {Foundations of {{Genetic Algorithms}}},
  author = {Eshelman, Larry J.},
  editor = {Rawlins, GREGORY J. E.},
  year = {1991},
  month = jan,
  volume = {1},
  pages = {265--283},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-08-050684-5.50020-3},
  urldate = {2023-05-31},
  abstract = {This paper describes and analyzes CHC, a nontraditional genetic algorithm which combines a conservative selection strategy that always preserves the best individuals found so far with a radical (highly disruptive) recombination operator that produces offspring that are maximally different from both parents. The traditional reasons for preferring a recombination operator with a low probability of disrupting schemata may not hold when such a conservative selection strategy is used. On the contrary, certain highly disruptive crossover operators provide more effective search. Empirical evidence is provided to support these claims.},
  langid = {english},
  keywords = {cross-generational competition,elitist selection,implicit parallelism,incest prevention,restarts,uniform crossover}
}

@article{latorrePrescriptionMethodologicalGuidelines2021,
  title = {A Prescription of Methodological Guidelines for Comparing Bio-Inspired Optimization Algorithms},
  author = {LaTorre, Antonio and Molina, Daniel and Osaba, Eneko and Poyatos, Javier and Del Ser, Javier and Herrera, Francisco},
  year = {2021},
  month = dec,
  journal = {Swarm and Evolutionary Computation},
  volume = {67},
  pages = {100973},
  issn = {2210-6502},
  doi = {10.1016/j.swevo.2021.100973},
  urldate = {2023-05-31},
  abstract = {Bio-inspired optimization (including Evolutionary Computation and Swarm Intelligence) is a growing research topic with many competitive bio-inspired algorithms being proposed every year. In such an active area, preparing a successful proposal of a new bio-inspired algorithm is not an easy task. Given the maturity of this research field, proposing a new optimization technique with innovative elements is no longer enough. Apart from the novelty, results reported by the authors should be proven to achieve a significant advance over previous outcomes from the state of the art. Unfortunately, not all new proposals deal with this requirement properly. Some of them fail to select appropriate benchmarks or reference algorithms to compare with. In other cases, the validation process carried out is not defined in a principled way (or is even not done at all). Consequently, the significance of the results presented in such studies cannot be guaranteed. In this work we review several recommendations in the literature and propose methodological guidelines to prepare a successful proposal, taking all these issues into account. We expect these guidelines to be useful not only for authors, but also for reviewers and editors along their assessment of new contributions to the field.},
  langid = {english},
  keywords = {Benchmarking,Bio-inspired optimization,Comparison methodologies,Guidelines,Parameter tuning,Recommendations review,Statistical analysis},
  note = {43 citations (Crossref) [2023-05-31]}
}

@article{liEvolutionaryComputationExpensive2022,
  title = {Evolutionary {{Computation}} for {{Expensive Optimization}}: {{A Survey}}},
  shorttitle = {Evolutionary {{Computation}} for {{Expensive Optimization}}},
  author = {Li, Jian-Yu and Zhan, Zhi-Hui and Zhang, Jun},
  year = {2022},
  month = feb,
  journal = {Machine Intelligence Research},
  volume = {19},
  number = {1},
  pages = {3--23},
  issn = {2731-5398},
  doi = {10.1007/s11633-022-1317-4},
  urldate = {2023-05-31},
  abstract = {Expensive optimization problem (EOP) widely exists in various significant real-world applications. However, EOP requires expensive or even unaffordable costs for evaluating candidate solutions, which is expensive for the algorithm to find a satisfactory solution. Moreover, due to the fast-growing application demands in the economy and society, such as the emergence of the smart cities, the internet of things, and the big data era, solving EOP more efficiently has become increasingly essential in various fields, which poses great challenges on the problem-solving ability of optimization approach for EOP. Among various optimization approaches, evolutionary computation (EC) is a promising global optimization tool widely used for solving EOP efficiently in the past decades. Given the fruitful advancements of EC for EOP, it is essential to review these advancements in order to synthesize and give previous research experiences and references to aid the development of relevant research fields and real-world applications. Motivated by this, this paper aims to provide a comprehensive survey to show why and how EC can solve EOP efficiently. For this aim, this paper firstly analyzes the total optimization cost of EC in solving EOP. Then, based on the analysis, three promising research directions are pointed out for solving EOP, which are problem approximation and substitution, algorithm design and enhancement, and parallel and distributed computation. Note that, to the best of our knowledge, this paper is the first that outlines the possible directions for efficiently solving EOP by analyzing the total expensive cost. Based on this, existing works are reviewed comprehensively via a taxonomy with four parts, including the above three research directions and the real-world application part. Moreover, some future research directions are also discussed in this paper. It is believed that such a survey can attract attention, encourage discussions, and stimulate new EC research ideas for solving EOP and related real-world applications more efficiently.},
  langid = {english},
  keywords = {differential evolution,evolutionary algorithm,evolutionary computation,Expensive optimization problem,particle swarm optimization,swarm intelligence},
  note = {21 citations (Crossref) [2023-05-31]}
}

@article{martinezLightsShadowsEvolutionary2021b,
  title = {Lights and Shadows in {{Evolutionary Deep Learning}}: {{Taxonomy}}, Critical Methodological Analysis, Cases of Study, Learned Lessons, Recommendations and Challenges},
  shorttitle = {Lights and Shadows in {{Evolutionary Deep Learning}}},
  author = {Martinez, Aritz D. and Del Ser, Javier and {Villar-Rodriguez}, Esther and Osaba, Eneko and Poyatos, Javier and Tabik, Siham and Molina, Daniel and Herrera, Francisco},
  year = {2021},
  month = mar,
  journal = {Information Fusion},
  volume = {67},
  pages = {161--194},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2020.10.014},
  urldate = {2023-05-31},
  abstract = {Much has been said about the fusion of bio-inspired optimization algorithms and Deep Learning models for several purposes: from the discovery of network topologies and hyperparametric configurations with improved performance for a given task, to the optimization of the model's parameters as a replacement for gradient-based solvers. Indeed, the literature is rich in proposals showcasing the application of assorted nature-inspired approaches for these tasks. In this work we comprehensively review and critically examine contributions made so far based on three axes, each addressing a fundamental question in this research avenue: (a) optimization and taxonomy (Why?), including a historical perspective, definitions of optimization problems in Deep Learning, and a taxonomy associated with an in-depth analysis of the literature, (b) critical methodological analysis (How?), which together with two case studies, allows us to address learned lessons and recommendations for good practices following the analysis of the literature, and (c) challenges and new directions of research (What can be done, and what for?). In summary, three axes \textendash{} optimization and taxonomy, critical analysis, and challenges \textendash{} which outline a complete vision of a merger of two technologies drawing up an exciting future for this area of fusion research.},
  langid = {english},
  keywords = {Deep Learning,Evolutionary Computation,Neuroevolution,Swarm Intelligence},
  note = {12 citations (Crossref) [2023-05-31]}
}

@inproceedings{matsuiNewSelectionMethod1999a,
  title = {New Selection Method to Improve the Population Diversity in Genetic Algorithms},
  booktitle = {{{IEEE SMC}}'99 {{Conference Proceedings}}. 1999 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{Cat}}. {{No}}.{{99CH37028}})},
  author = {Matsui, K.},
  year = {1999},
  month = oct,
  volume = {1},
  pages = {625-630 vol.1},
  issn = {1062-922X},
  doi = {10.1109/ICSMC.1999.814164},
  abstract = {We present a new method of selection, in order to improve the population diversity in the genotype distribution, in genetic algorithms (GAs). The problem of maintaining of the population diversity is very important in designing genetic operators, when GAs are applied to optimization problems. Therefore, we propose two types of new selection operators based on the correlations between individuals' genotypes, for improving the population diversity. The first operator is a new type of selection for reproduction, namely the correlative tournament selection. The second operator is a new type of selection for survival, namely correlative family-based selection. We have applied our GA to two different problems: Royal road problems, and Knapsack problems with non-stationary environments. We have compared our method with the other representative GA model, and have shown the effectiveness of the proposed GA models.},
  keywords = {Biological system modeling,Biology computing,Computational modeling,Design optimization,Erbium,Genetic algorithms,Optimization methods,Process design,Stochastic processes,Testing},
  note = {14 citations (Crossref) [2023-05-31]}
}

@book{michalewiczHandbookEvolutionaryComputation1997,
  title = {Handbook of {{Evolutionary Computation}}},
  editor = {Michalewicz, D. B. Fogel, Thomas Baeck, Z.},
  year = {1997},
  month = jan,
  publisher = {{CRC Press}},
  address = {{Boca Raton}},
  doi = {10.1201/9780367802486},
  abstract = {Many scientists and engineers now use the paradigms of evolutionary computation (genetic algorithms, evolution strategies, evolutionary programming, genetic programming, classifier systems, and combinations or hybrids) to tackle problems that are either intractable or unrealistically time consuming to solve through traditional computational strategies. The Handbook of Evolutionary Computation addresses the need for a comprehensive source of reference in the maturing field of evolutionary computation. The handbook is available in a looseleaf print format and an online format.},
  isbn = {978-0-367-80248-6},
  file = {C\:\\Users\\irene\\Zotero\\storage\\BUQU67PT\\Michalewicz, D. B. Fogel - 1997 - Handbook of Evolutionary Computation.pdf}
}

@article{poyatosEvoPruneDeepTLEvolutionaryPruning2023a,
  title = {{{EvoPruneDeepTL}}: {{An}} Evolutionary Pruning Model for Transfer Learning Based Deep Neural Networks},
  shorttitle = {{{EvoPruneDeepTL}}},
  author = {Poyatos, Javier and Molina, Daniel and Martinez, Aritz D. and Del Ser, Javier and Herrera, Francisco},
  year = {2023},
  month = jan,
  journal = {Neural Networks},
  volume = {158},
  pages = {59--82},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2022.10.011},
  urldate = {2023-05-31},
  abstract = {In recent years, Deep Learning models have shown a great performance in complex optimization problems. They generally require large training datasets, which is a limitation in most practical cases. Transfer learning allows importing the first layers of a pre-trained architecture and connecting them to fully-connected layers to adapt them to a new problem. Consequently, the configuration of the these layers becomes crucial for the performance of the model. Unfortunately, the optimization of these models is usually a computationally demanding task. One strategy to optimize Deep Learning models is the pruning scheme. Pruning methods are focused on reducing the complexity of the network, assuming an expected performance penalty of the model once pruned. However, the pruning could potentially be used to improve the performance, using an optimization algorithm to identify and eventually remove unnecessary connections among neurons. This work proposes EvoPruneDeepTL, an evolutionary pruning model for Transfer Learning based Deep Neural Networks which replaces the last fully-connected layers with sparse layers optimized by a genetic algorithm. Depending on its solution encoding strategy, our proposed model can either perform optimized pruning or feature selection over the densely connected part of the neural network. We carry out different experiments with several datasets to assess the benefits of our proposal. Results show the contribution of EvoPruneDeepTL and feature selection to the overall computational efficiency of the network as a result of the optimization process. In particular, the accuracy is improved, reducing at the same time the number of active neurons in the final layers.},
  langid = {english},
  keywords = {Deep learning,Evolutionary algorithms,Feature selection,Pruning,Transfer learning},
  note = {1 citations (Crossref) [2023-05-31]}
}
