@inproceedings{backEvolutionaryComputationOverview1996,
  title = {Evolutionary Computation: {{An}} Overview},
  shorttitle = {Evolutionary Computation},
  booktitle = {Proceedings of {{IEEE International Conference}} on {{Evolutionary Computation}}},
  author = {Back, T. and Schwefel, H.-P.},
  year = {1996},
  month = may,
  pages = {20--29},
  doi = {10.1109/ICEC.1996.542329},
  abstract = {We present an overview of the most important representatives of algorithms gleaned from natural evolution, so-called evolutionary algorithms. Evolution strategies, evolutionary programming, and genetic algorithms are summarized, with special emphasis on the principle of strategy parameter self-adaptation utilized by the first two algorithms to learn their own strategy parameters such as mutation variances and covariances. Some experimental results are presented which demonstrate the working principle and robustness of the self-adaptation methods used in evolution strategies and evolutionary programming. General principles of evolutionary algorithms are discussed, and we identify certain properties of natural evolution which might help to improve the problem solving capabilities of evolutionary algorithms even further.},
  keywords = {Algorithm design and analysis,Europe,Evolution (biology),Evolutionary computation,Genetic algorithms,Genetic mutations,Genetic programming,Optimization methods,Problem-solving,Robustness},
  note = {105 citations (Crossref) [2023-05-31]}
}

@book{banzhafGeneticProgrammingIntroduction1998,
  title = {Genetic Programming: An Introduction on the Automatic Evolution of Computer Programs and Its Applications},
  shorttitle = {Genetic Programming},
  editor = {Banzhaf, Wolfgang},
  year = {1998},
  publisher = {{Morgan Kaufmann Publishers ; Dpunkt-verlag}},
  address = {{San Francisco, Calif. : Heidelburg}},
  isbn = {978-1-55860-510-7 978-3-920993-58-4},
  langid = {english},
  lccn = {QA76.623 .G46 1998},
  keywords = {Genetic programming (Computer science)},
  file = {C\:\\Users\\irene\\Zotero\\storage\\ZESAUKXS\\Banzhaf - 1998 - Genetic programming an introduction on the automa.pdf}
}

@article{buiMetaheuristicAlgorithmsOptimizing2019,
  title = {Metaheuristic Algorithms in Optimizing Neural Network: A Comparative Study for Forest Fire Susceptibility Mapping in {{Dak Nong}}, {{Vietnam}}},
  shorttitle = {Metaheuristic Algorithms in Optimizing Neural Network},
  author = {Bui, Quang-Thanh},
  year = {2019},
  month = jan,
  journal = {Geomatics, Natural Hazards and Risk},
  volume = {10},
  number = {1},
  pages = {136--150},
  issn = {1947-5705, 1947-5713},
  doi = {10.1080/19475705.2018.1509902},
  urldate = {2023-05-18},
  abstract = {Meta-heuristic algorithms become common approaches in finding sufficiently good solutions for optimization problems. This study proposed and compared three novel hybrid methods, namely Biogeography-based Optimization (BBO), Gravitational Search Algorithm (GSA) and Grey Wolf Optimization (GWO) in combination with the popular Neural Network classifier for forest fire modeling. Dak Nong province was selected as a case study as it had undergone a critical drought season. One thousand three hundred and thirty-eight historic fired locations during the first several months of 2017 were chosen as dependent variables. On the other hand, topological, climatic and socio-economic data were used as independent predictor variables. For accuracy assessment, root mean square error derivable from the neural network was used as an objective function to be optimized by three proposed algorithms. The results showed that the area under Receiver Operating Characteristic curves (AUC) were in BBO (0.9515), GWO (0.9509), (0.9398) outperformed the Regular neural with backpropagation algorithm (AUC = 0.9271). Even though the differences between prediction results were small, but they were significant by using a paired t-test. It could be concluded that three hybrid models are suitable to map forest fire susceptibility in the selected study area and could be considered as alternative methods for studying forest fire in other locations.},
  langid = {english},
  file = {C\:\\Users\\irene\\Zotero\\storage\\KQCIZW7W\\Bui - 2019 - Metaheuristic algorithms in optimizing neural netw.pdf}
}

@article{chughSurveyHandlingComputationally2019,
  title = {A Survey on Handling Computationally Expensive Multiobjective Optimization Problems with Evolutionary Algorithms},
  author = {Chugh, Tinkle and Sindhya, Karthik and Hakanen, Jussi and Miettinen, Kaisa},
  year = {2019},
  month = may,
  journal = {Soft Computing},
  volume = {23},
  doi = {10.1007/s00500-017-2965-0},
  abstract = {Evolutionary algorithms are widely used for solving multiobjective optimization problems but are often criticized because of a large number of function evaluations needed. Approximations, especially function approximations, also referred to as surrogates or metamodels are commonly used in the literature to reduce the computation time. This paper presents a survey of 45 different recent algorithms proposed in the literature between 2008 and 2016 to handle computationally expensive multiobjective optimization problems. Several algorithms are discussed based on what kind of an approximation such as problem, function or fitness approximation they use. Most emphasis is given to function approximation-based algorithms. We also compare these algorithms based on different criteria such as metamodeling technique and evolutionary algorithm used, type and dimensions of the problem solved, handling constraints, training time and the type of evolution control. Furthermore, we identify and discuss some promising elements and major issues among algorithms in the literature related to using an approximation and numerical settings used. In addition, we discuss selecting an algorithm to solve a given computationally expensive multiobjective optimization problem based on the dimensions in both objective and decision spaces and the computation budget available.},
  file = {C\:\\Users\\irene\\Zotero\\storage\\8GWHM2J2\\Chugh et al. - 2019 - A survey on handling computationally expensive mul.pdf}
}

@article{colakNEURALNETWORKSBASED,
  title = {{{NEURAL NETWORKS BASED META-HEURISTICS FOR SOLVING OPTIMIZATION PROBLEMS}}},
  author = {Colak, Selcuk},
  langid = {english},
  file = {C\:\\Users\\irene\\Zotero\\storage\\2PX6G8J5\\Colak - NEURAL NETWORKS BASED META-HEURISTICS FOR SOLVING .pdf}
}

@article{demoraesDiversityPreservationMethod2022,
  title = {A Diversity Preservation Method for Expensive Multi-Objective Combinatorial Optimization Problems Using {{Novel-First Tabu Search}} and {{MOEA}}/{{D}}},
  author = {{de Moraes}, Matheus Bernardelli and Coelho, Guilherme Palermo},
  year = {2022},
  month = sep,
  journal = {Expert Systems with Applications},
  volume = {202},
  pages = {117251},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2022.117251},
  urldate = {2023-06-02},
  abstract = {Expensive multi-objective combinatorial optimization problems have constraints in the number of objective function evaluations due to time, financial, or resource restrictions. As most combinatorial problems, they are subject to a high number of duplicated solutions. Given the fact that expensive environments limit the number of objective function evaluations, the existence of duplicated solutions heavily impacts the optimization process due to poor diversity and low convergence speed. This paper proposes the Novel-First Tabu Search, a greedy-strategy mechanism that uses Knowledge-Assisted Local Search methods to preserve the population diversity and increase the exploration and exploitation ability of MOEA/D. Experiments are conducted on constrained, unconstrained, multimodal, deceptive, linear, convex, and non-convex Pareto Front multi-objective combinatorial optimization benchmark problems. This paper also conducts an experiment on the real-world, expensive problem of Well Placement Optimization using a benchmark case based on the Namorado oil field, located in the Campos Basin, Brazil. The experimental results and performance comparison with state-of-the-art algorithms demonstrate that the proposed design significantly preserves diversity and increases convergence without violating the constraint in the number of objective function evaluations.},
  langid = {english},
  keywords = {Black-box optimization,Decomposition-based methods,Diversity preservation,Expensive multi-objective combinatorial optimization,Novel-First Tabu Search},
  file = {C\:\\Users\\irene\\Zotero\\storage\\I6LKVEW9\\de Moraes y Coelho - 2022 - A diversity preservation method for expensive mult.pdf;C\:\\Users\\irene\\Zotero\\storage\\VXNU4ZQG\\S0957417422006261.html}
}

@article{derracPracticalTutorialUse2011,
  title = {A Practical Tutorial on the Use of Nonparametric Statistical Tests as a Methodology for Comparing Evolutionary and Swarm Intelligence Algorithms},
  author = {Derrac, Joaqu{\'i}n and Garc{\'i}a, Salvador and Molina, Daniel and Herrera, Francisco},
  year = {2011},
  month = mar,
  journal = {Swarm and Evolutionary Computation},
  volume = {1},
  number = {1},
  pages = {3--18},
  issn = {22106502},
  doi = {10.1016/j.swevo.2011.02.002},
  urldate = {2023-05-20},
  abstract = {The interest in nonparametric statistical analysis has grown recently in the field of computational intelligence. In many experimental studies, the lack of the required properties for a proper application of parametric procedures \textendash{} independence, normality, and homoscedasticity \textendash{} yields to nonparametric ones the task of performing a rigorous comparison among algorithms.},
  langid = {english},
  note = {Test Estadisticos No param\'etricos
\par
Mirar la secci\'on 3 y 4 para rellenar el cap\'itulo de Contexto Matem\'atico respecto a los test estad\'isticos no param\'etricos},
  file = {C\:\\Users\\irene\\Zotero\\storage\\R3QTVWLN\\Derrac et al. - 2011 - A practical tutorial on the use of nonparametric s.pdf}
}

@article{ekExplorationExploitationEvolutionary,
  title = {Exploration and {{Exploitation}} in {{Evolutionary Algorithms}}: {{A Survey}}},
  author = {Ek, Matej C{\textasciicaron} Repins{\textasciicaron} and Liu, Shih-Hsi and Mernik, Marjan},
  journal = {ACM Computing Surveys},
  volume = {1},
  number = {1},
  langid = {english},
  file = {C\:\\Users\\irene\\Zotero\\storage\\SQLIPEWT\\Ek et al. - Exploration and Exploitation in Evolutionary Algor.pdf}
}

@incollection{eshelmanCHCAdaptiveSearch1991,
  title = {The {{CHC Adaptive Search Algorithm}}: {{How}} to {{Have Safe Search When Engaging}} in {{Nontraditional Genetic Recombination}}},
  shorttitle = {The {{CHC Adaptive Search Algorithm}}},
  booktitle = {Foundations of {{Genetic Algorithms}}},
  author = {Eshelman, Larry J.},
  editor = {Rawlins, GREGORY J. E.},
  year = {1991},
  month = jan,
  volume = {1},
  pages = {265--283},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-08-050684-5.50020-3},
  urldate = {2023-05-31},
  abstract = {This paper describes and analyzes CHC, a nontraditional genetic algorithm which combines a conservative selection strategy that always preserves the best individuals found so far with a radical (highly disruptive) recombination operator that produces offspring that are maximally different from both parents. The traditional reasons for preferring a recombination operator with a low probability of disrupting schemata may not hold when such a conservative selection strategy is used. On the contrary, certain highly disruptive crossover operators provide more effective search. Empirical evidence is provided to support these claims.},
  langid = {english},
  keywords = {cross-generational competition,elitist selection,implicit parallelism,incest prevention,restarts,uniform crossover}
}

@article{garcia-martinezStrategicOscillationQuadratic2014,
  title = {Strategic Oscillation for the Quadratic Multiple Knapsack Problem},
  author = {{Garc{\'i}a-Mart{\'i}nez}, Carlos and Glover, Fred and Rodriguez, Francisco J. and Lozano, Manuel and Mart{\'i}, Rafael},
  year = {2014},
  month = may,
  journal = {Computational Optimization and Applications},
  volume = {58},
  number = {1},
  pages = {161--185},
  issn = {0926-6003, 1573-2894},
  doi = {10.1007/s10589-013-9623-y},
  urldate = {2022-10-26},
  abstract = {The quadratic multiple knapsack problem (QMKP) consists in assigning a set of objects, which interact through paired profit values, exclusively to different capacity-constrained knapsacks with the aim of maximising total profit. Its many applications include the assignment of workmen to different tasks when their ability to cooperate may affect the results.},
  langid = {english},
  file = {C\:\\Users\\irene\\Zotero\\storage\\MQAWZ92B\\García-Martínez et al. - 2014 - Strategic oscillation for the quadratic multiple k.pdf}
}

@article{garzonTECNICASAVANZADAS,
  title = {{TE\textasciiacute CNICAS AVANZADAS DE HIBRIDACIO\textasciiacute{} N PARA METAHEUR\textasciiacute ISTICAS}},
  author = {Garzon, Andrea Morales and Cabrera, Daniel Molina},
  abstract = {In this project we will study the behavior of hybrid models which are composed of different combinations of evolutionary algorithms, mainly, basic genetic algorithms. For this study we will use different real-coding problems obtained from CEC'14 (Test Function Suite Optimization). We will analyze the behavior of these algorithms for some of the functions of different characteristics: unimodal and multimodal functions, hybrid function, and other type of functions as result of a composition of previous functions working together.},
  langid = {spanish},
  file = {C\:\\Users\\irene\\Zotero\\storage\\BEG9NPWY\\Garzon y Cabrera - TE´CNICAS AVANZADAS DE HIBRIDACIO´ N PARA METAHEUR.pdf}
}

@article{gershmanDeconstructingHumanAlgorithms2018,
  title = {Deconstructing the Human Algorithms for Exploration},
  author = {Gershman, Samuel J.},
  year = {2018},
  month = apr,
  journal = {Cognition},
  volume = {173},
  pages = {34--42},
  issn = {00100277},
  doi = {10.1016/j.cognition.2017.12.014},
  urldate = {2023-03-12},
  abstract = {The dilemma between information gathering (exploration) and reward seeking (exploitation) is a fundamental problem for reinforcement learning agents. How humans resolve this dilemma is still an open question, because experiments have provided equivocal evidence about the underlying algorithms used by humans. We show that two families of algorithms can be distinguished in terms of how uncertainty affects exploration. Algorithms based on uncertainty bonuses predict a change in response bias as a function of uncertainty, whereas algorithms based on sampling predict a change in response slope. Two experiments provide evidence for both bias and slope changes, and computational modeling confirms that a hybrid model is the best quantitative account of the data.},
  langid = {english},
  file = {C\:\\Users\\irene\\Zotero\\storage\\W6E56GWP\\Gershman - 2018 - Deconstructing the human algorithms for exploratio.pdf}
}

@misc{herrera-poyatosGeneticMemeticAlgorithm2017,
  title = {Genetic and {{Memetic Algorithm}} with {{Diversity Equilibrium}} Based on {{Greedy Diversification}}},
  author = {{Herrera-Poyatos}, Andr{\'e}s and Herrera, Francisco},
  year = {2017},
  month = feb,
  number = {arXiv:1702.03594},
  eprint = {1702.03594},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-02-18},
  abstract = {The lack of diversity in a genetic algorithm's population may lead to a bad performance of the genetic operators since there is not an equilibrium between exploration and exploitation. In those cases, genetic algorithms present a fast and unsuitable convergence. In this paper we develop a novel hybrid genetic algorithm which attempts to obtain a balance between exploration and exploitation. It confronts the diversity problem using the named greedy diversification operator. Furthermore, the proposed algorithm applies a competition between parent and children so as to exploit the high quality visited solutions. These operators are complemented by a simple selection mechanism designed to preserve and take advantage of the population diversity.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,I.2.8},
  note = {\section{Anotaciones\\
(13/3/2023 1:24:59)}

\par
"Greedy diversification operator" (Herrera-Poyatos y Herrera, 2017, p. 6) La diversidad de la poblaci\'on es una hoja de doble filo. Por lo que, es deseable un operador de diversificaci\'on que solo introduzca diversidad si es necesario
\par
"Population diversity is a double-edged sword" (Herrera-Poyatos y Herrera, 2017, p. 6)
\par
"Therefore, it is desired a diversification operator that only introduces diversity if it is necessary." (Herrera-Poyatos y Herrera, 2017, p. 6)
\par
"The diversification operator should delete the population's repeated chromosomes" (Herrera-Poyatos y Herrera, 2017, p. 6)
\par
"the chromosomes that are left in the population should have a good objective value and be potentially good for the crossover operator" (Herrera-Poyatos y Herrera, 2017, p. 6)
\par
"The diversification operator ought also to have a low computational cost" (Herrera-Poyatos y Herrera, 2017, p. 6)
\par
"We propose using a greedy randomized algorithm" (Herrera-Poyatos y Herrera, 2017, p. 6) GRASP
\par
"Greedy randomized algorithms provide acceptable chromosomes from the objective value perspective that also contain high quality genetic material thanks to the greedy selection function. The randomized aspect of the algorithm supplies the diversity required in the generated solutions" (Herrera-Poyatos y Herrera, 2017, p. 6) Razonamiento del uso de GRASP
\par
"elements whose greedy value is less than (1 + {$\sigma$}) times the best element's value," (Herrera-Poyatos y Herrera, 2017, p. 6) \textquestiondown Qu\'e elementos elegir para el GRASP?
\par
"This model obtains better solutions because it controls the quality of the elements added to the list. It also keeps the diversity in the generated solutions" (Herrera-Poyatos y Herrera, 2017, p. 6)
\par
"we use {$\sigma$} = 0.1 although this parameter can be optimized in each application domain" (Herrera-Poyatos y Herrera, 2017, p. 7)
\par
"the first element of our proposal is a diversification operator which uses the greedy randomized algorithm to substitute those chromosomes that share similarity characteristics with other population solutions" (Herrera-Poyatos y Herrera, 2017, p. 7)
\par
"This procedure increase the diversity and also keeps the population quality" (Herrera-Poyatos y Herrera, 2017, p. 7)
\par
"greedy diversification operator is used instead of the mutation operator" (Herrera-Poyatos y Herrera, 2017, p. 8) El operador de diversificaci\'on Greedy (GRASP) se usa en vez del operador de mutaci\'on
\par
"There are also models which work under small populations" (Herrera-Poyatos y Herrera, 2017, p. 10) Hay modelos que trabajan con poblaciones peque\~nas -{$>$} pedir archivo
\par
"Micro-genetic algorithms for stationary and non-stationary function optimization" (Herrera-Poyatos y Herrera, 2017, p. 26) Cuando tratamos con un problemas con alta dimensionalidad, puede ser dif\'icil o costoso en t\'erminos de tiempo para todos los par\'ametros del modelo converger dentro de un margen de error dado. En particular, cuando el n\'umero de par\'ametros del modelo aumenta, tambi\'en lo hace el tama\~no de poblaci\'on requerida.

Un tama\~no de poblaci\'on grande implica grandes n\'umeros de evaluaci\'on-coste.\\
Una alternativa es el uso de micro algoritmos gen\'eticos, que evolucionan poblaciones muy peque\~nas queson muy eficientes localizando \'areas prometedoras del espacio de b\'usqueda. Obviamente, las poblaciones peque\~nas son incapaces de mantener la diversidad por muchas generaciones, pero la poblaci\'on puede ser reiniciada cada vez que la poblaci\'on se pierde, manteniendo solo la mejor soluci\'on obtenida.

Reiniciar la poblaci\'on muchas veces durante la ejecuci\'on del algoritmo gen\'etico tiene el beneficio a\~nadido de evitar la convergencia prematura debido a la presencia de un individuo con un buen valor, lo que posee el riesgo de prevenir m\'as exploraciones del espacio de b\'usqueda y haga converger al programa a un m\'inimo local.

Adem\'as, en tanto que no estamos tratando con poblaciones largos, la convergencia puede ser alcanzada m\'as r\'apidamente y con menos requisitos de memoria para almacenar la poblaci\'on
\par
Comment: 27 pages, 5 figures, 11 tables},
  file = {C\:\\Users\\irene\\Zotero\\storage\\D5DHGSGV\\Herrera-Poyatos y Herrera - 2017 - Genetic and Memetic Algorithm with Diversity Equil}
}

@article{iaccaOckhamRazorMemetic2012,
  title = {Ockham's {{Razor}} in Memetic Computing: {{Three}} Stage Optimal Memetic Exploration},
  shorttitle = {Ockham's {{Razor}} in Memetic Computing},
  author = {Iacca, Giovanni and Neri, Ferrante and Mininno, Ernesto and Ong, Yew-Soon and Lim, Meng-Hiot},
  year = {2012},
  month = apr,
  journal = {Information Sciences},
  volume = {188},
  pages = {17--43},
  issn = {00200255},
  doi = {10.1016/j.ins.2011.11.025},
  urldate = {2023-03-12},
  abstract = {Memetic computing is a subject in computer science which considers complex structures as the combination of simple agents, memes, whose evolutionary interactions lead to intelligent structures capable of problem-solving. This paper focuses on memetic computing optimization algorithms and proposes a counter-tendency approach for algorithmic design. Research in the field tends to go in the direction of improving existing algorithms by combining different methods or through the formulation of more complicated structures. Contrary to this trend, we instead focus on simplicity, proposing a structurally simple algorithm with emphasis on processing only one solution at a time. The proposed algorithm, namely three stage optimal memetic exploration, is composed of three memes; the first stochastic and with a long search radius, the second stochastic and with a moderate search radius and the third deterministic and with a short search radius. The bottom-up combination of the three operators by means of a natural trial and error logic, generates a robust and efficient optimizer, capable of competing with modern complex and computationally expensive algorithms. This is suggestive of the fact that complexity in algorithmic structures can be unnecessary, if not detrimental, and that simple bottom-up approaches are likely to be competitive is here invoked as an extension to memetic computing basing on the philosophical concept of Ockham's Razor. An extensive experimental setup on various test problems and one digital signal processing application is presented. Numerical results show that the proposed approach, despite its simplicity and low computational cost displays a very good performance on several problems, and is competitive with sophisticated algorithms representing the-state-of-the-art in computational intelligence optimization.},
  langid = {english},
  file = {C\:\\Users\\irene\\Zotero\\storage\\7UNTBKYT\\Iacca et al. - 2012 - Ockham’s Razor in memetic computing Three stage o.pdf}
}

@book{islamMATHEMATICALPROGRAMMING2020,
  title = {{{MATHEMATICAL PROGRAMMING}}},
  author = {Islam, S.},
  year = {2020},
  month = jan,
  volume = {1},
  pages = {700},
  abstract = {This book consists of the preliminaries of mathematical programming, convex sets, topics of linear programming, integer linear programming, transportation problem, assignment problem and the basic of optimization using calculus method. Every topic has been presented in a simple way. The book becomes easier to understand for the readers, because, we follow the method definition-example-theory-example-application in writing each section. Students get here some done examples, mostly collected from the question papers of different universities. Exercises are included for home work. By studying the book, readers will learn the applications as well as the basic understanding of mathematical programming.},
  isbn = {978-620-0-46432-3},
  file = {C\:\\Users\\irene\\Zotero\\storage\\LF4BD6K2\\Islam - 2020 - MATHEMATICAL PROGRAMMING.pdf}
}

@inproceedings{joyMicroGeneticAlgorithmEmbedded2021,
  title = {Micro-{{Genetic Algorithm Embedded Multi-Population Differential Evolution}} Based {{Neural Network}} for {{Short-Term Load Forecasting}}},
  booktitle = {2021 56th {{International Universities Power Engineering Conference}} ({{UPEC}})},
  author = {Joy, Colin Paul and Pillai, Gobind and Chen, Yingke and Mistry, Kamlesh},
  year = {2021},
  month = aug,
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{Middlesbrough, United Kingdom}},
  doi = {10.1109/UPEC50034.2021.9548262},
  urldate = {2023-03-15},
  abstract = {The load of a power system usually presents a certain range of nonlinear fluctuation with time. Even then, the load characteristics still follow certain rules which can be exploited to optimise and improve the accuracy of computerbased Short-Term Load Forecasting (STLF) models. Therefore, this paper presents a mGA (micro\textendash Genetic Algorithm) embedded multi-population DE (Differential Evolution) to optimise an Artificial Neural Network (ANN) STLF model. Firstly, the mGA embedded multi-population DE is proposed, to improve and balance the global and local search. Then the proposed DE is applied to optimise the weights during the training of the ANN. The overall model's performance is evaluated using publicly available Panama electricity load dataset against four state-of-the-art machine learning algorithms. The evaluation results show that the proposed DE based NN STLF model has higher prediction accuracy compared to the other selected machine learning algorithms.},
  isbn = {978-1-66544-389-0},
  langid = {english},
  file = {C\:\\Users\\irene\\Zotero\\storage\\MT3VUCJM\\Joy et al. - 2021 - Micro-Genetic Algorithm Embedded Multi-Population .pdf}
}

@article{latorrePrescriptionMethodologicalGuidelines2021,
  title = {A Prescription of Methodological Guidelines for Comparing Bio-Inspired Optimization Algorithms},
  author = {LaTorre, Antonio and Molina, Daniel and Osaba, Eneko and Poyatos, Javier and Del Ser, Javier and Herrera, Francisco},
  year = {2021},
  month = dec,
  journal = {Swarm and Evolutionary Computation},
  volume = {67},
  pages = {100973},
  issn = {2210-6502},
  doi = {10.1016/j.swevo.2021.100973},
  urldate = {2023-05-31},
  abstract = {Bio-inspired optimization (including Evolutionary Computation and Swarm Intelligence) is a growing research topic with many competitive bio-inspired algorithms being proposed every year. In such an active area, preparing a successful proposal of a new bio-inspired algorithm is not an easy task. Given the maturity of this research field, proposing a new optimization technique with innovative elements is no longer enough. Apart from the novelty, results reported by the authors should be proven to achieve a significant advance over previous outcomes from the state of the art. Unfortunately, not all new proposals deal with this requirement properly. Some of them fail to select appropriate benchmarks or reference algorithms to compare with. In other cases, the validation process carried out is not defined in a principled way (or is even not done at all). Consequently, the significance of the results presented in such studies cannot be guaranteed. In this work we review several recommendations in the literature and propose methodological guidelines to prepare a successful proposal, taking all these issues into account. We expect these guidelines to be useful not only for authors, but also for reviewers and editors along their assessment of new contributions to the field.},
  langid = {english},
  keywords = {Benchmarking,Bio-inspired optimization,Comparison methodologies,Guidelines,Parameter tuning,Recommendations review,Statistical analysis},
  note = {43 citations (Crossref) [2023-05-31]}
}

@article{liEvolutionaryComputationExpensive2022,
  title = {Evolutionary {{Computation}} for {{Expensive Optimization}}: {{A Survey}}},
  shorttitle = {Evolutionary {{Computation}} for {{Expensive Optimization}}},
  author = {Li, Jian-Yu and Zhan, Zhi-Hui and Zhang, Jun},
  year = {2022},
  month = feb,
  journal = {Machine Intelligence Research},
  volume = {19},
  number = {1},
  pages = {3--23},
  issn = {2731-538X, 2731-5398},
  doi = {10.1007/s11633-022-1317-4},
  urldate = {2022-10-05},
  abstract = {Expensive optimization problem (EOP) widely exists in various significant real-world applications. However, EOP requires expensive or even unaffordable costs for evaluating candidate solutions, which is expensive for the algorithm to find a satisfactory solution. Moreover, due to the fast-growing application demands in the economy and society, such as the emergence of the smart cities, the internet of things, and the big data era, solving EOP more efficiently has become increasingly essential in various fields, which poses great challenges on the problem-solving ability of optimization approach for EOP. Among various optimization approaches, evolutionary computation (EC) is a promising global optimization tool widely used for solving EOP efficiently in the past decades. Given the fruitful advancements of EC for EOP, it is essential to review these advancements in order to synthesize and give previous research experiences and references to aid the development of relevant research fields and real-world applications. Motivated by this, this paper aims to provide a comprehensive survey to show why and how EC can solve EOP efficiently. For this aim, this paper firstly analyzes the total optimization cost of EC in solving EOP. Then, based on the analysis, three promising research directions are pointed out for solving EOP, which are problem approximation and substitution, algorithm design and enhancement, and parallel and distributed computation. Note that, to the best of our knowledge, this paper is the first that outlines the possible directions for efficiently solving EOP by analyzing the total expensive cost. Based on this, existing works are reviewed comprehensively via a taxonomy with four parts, including the above three research directions and the real-world application part. Moreover, some future research directions are also discussed in this paper. It is believed that such a survey can attract attention, encourage discussions, and stimulate new EC research ideas for solving EOP and related real-world applications more efficiently.},
  langid = {english},
  note = {Definici\'on EOP
\par
\textquestiondown Ejemplos?},
  file = {C\:\\Users\\irene\\Zotero\\storage\\V2I6X9CN\\Li et al. - 2022 - Evolutionary Computation for Expensive Optimizatio.pdf}
}

@article{martinezLightsShadowsEvolutionary2021b,
  title = {Lights and Shadows in {{Evolutionary Deep Learning}}: {{Taxonomy}}, Critical Methodological Analysis, Cases of Study, Learned Lessons, Recommendations and Challenges},
  shorttitle = {Lights and Shadows in {{Evolutionary Deep Learning}}},
  author = {Martinez, Aritz D. and Del Ser, Javier and {Villar-Rodriguez}, Esther and Osaba, Eneko and Poyatos, Javier and Tabik, Siham and Molina, Daniel and Herrera, Francisco},
  year = {2021},
  month = mar,
  journal = {Information Fusion},
  volume = {67},
  pages = {161--194},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2020.10.014},
  urldate = {2023-05-31},
  abstract = {Much has been said about the fusion of bio-inspired optimization algorithms and Deep Learning models for several purposes: from the discovery of network topologies and hyperparametric configurations with improved performance for a given task, to the optimization of the model's parameters as a replacement for gradient-based solvers. Indeed, the literature is rich in proposals showcasing the application of assorted nature-inspired approaches for these tasks. In this work we comprehensively review and critically examine contributions made so far based on three axes, each addressing a fundamental question in this research avenue: (a) optimization and taxonomy (Why?), including a historical perspective, definitions of optimization problems in Deep Learning, and a taxonomy associated with an in-depth analysis of the literature, (b) critical methodological analysis (How?), which together with two case studies, allows us to address learned lessons and recommendations for good practices following the analysis of the literature, and (c) challenges and new directions of research (What can be done, and what for?). In summary, three axes \textendash{} optimization and taxonomy, critical analysis, and challenges \textendash{} which outline a complete vision of a merger of two technologies drawing up an exciting future for this area of fusion research.},
  langid = {english},
  keywords = {Deep Learning,Evolutionary Computation,Neuroevolution,Swarm Intelligence},
  note = {12 citations (Crossref) [2023-05-31]}
}

@inproceedings{matsuiNewSelectionMethod1999a,
  title = {New Selection Method to Improve the Population Diversity in Genetic Algorithms},
  booktitle = {{{IEEE SMC}}'99 {{Conference Proceedings}}. 1999 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{Cat}}. {{No}}.{{99CH37028}})},
  author = {Matsui, K.},
  year = {1999},
  month = oct,
  volume = {1},
  pages = {625-630 vol.1},
  issn = {1062-922X},
  doi = {10.1109/ICSMC.1999.814164},
  abstract = {We present a new method of selection, in order to improve the population diversity in the genotype distribution, in genetic algorithms (GAs). The problem of maintaining of the population diversity is very important in designing genetic operators, when GAs are applied to optimization problems. Therefore, we propose two types of new selection operators based on the correlations between individuals' genotypes, for improving the population diversity. The first operator is a new type of selection for reproduction, namely the correlative tournament selection. The second operator is a new type of selection for survival, namely correlative family-based selection. We have applied our GA to two different problems: Royal road problems, and Knapsack problems with non-stationary environments. We have compared our method with the other representative GA model, and have shown the effectiveness of the proposed GA models.},
  keywords = {Biological system modeling,Biology computing,Computational modeling,Design optimization,Erbium,Genetic algorithms,Optimization methods,Process design,Stochastic processes,Testing},
  note = {14 citations (Crossref) [2023-05-31]}
}

@book{michalewiczHandbookEvolutionaryComputation1997,
  title = {Handbook of {{Evolutionary Computation}}},
  editor = {Michalewicz, D. B. Fogel, Thomas Baeck, Z.},
  year = {1997},
  month = jan,
  publisher = {{CRC Press}},
  address = {{Boca Raton}},
  doi = {10.1201/9780367802486},
  abstract = {Many scientists and engineers now use the paradigms of evolutionary computation (genetic algorithms, evolution strategies, evolutionary programming, genetic programming, classifier systems, and combinations or hybrids) to tackle problems that are either intractable or unrealistically time consuming to solve through traditional computational strategies. The Handbook of Evolutionary Computation addresses the need for a comprehensive source of reference in the maturing field of evolutionary computation. The handbook is available in a looseleaf print format and an online format.},
  isbn = {978-0-367-80248-6},
  file = {C\:\\Users\\irene\\Zotero\\storage\\BUQU67PT\\Michalewicz, D. B. Fogel - 1997 - Handbook of Evolutionary Computation.pdf}
}

@article{oppacherShiftingBalanceGenetic,
  title = {The {{Shifting Balance Genetic Algorithm}}: {{Improving}} the {{GA}} in a {{Dynamic Environment}}},
  author = {Oppacher, Franz and Wineberg, Mark},
  abstract = {Two observed deficiencies of the GA are its tendency to get trapped at local maxima and the difficulty it has handling a changing environment after convergence has occurred. A mechanism proposed by Sewall Wright in the 1930s addresses the problem of premature convergence: his Shifting Balance Theory (SBT) of evolution. In this work the SBT has been modified to remove defects inherent in its original formulation, while keeping the properties that should both increase the adaptive abilities of the GA and prevent it from prematurely converging. The system has been implemented and is called the Shifting Balance Genetic Algorithm (SBGA). Experimental results and analysis are presented demonstrating that the SBGA does, in fact, lessen the problem of premature convergence and also improves performance under a dynamic environment, thereby mitigating both deficiencies.},
  langid = {english},
  file = {C\:\\Users\\irene\\Zotero\\storage\\YA44AL4J\\Oppacher y Wineberg - The Shifting Balance Genetic Algorithm Improving .pdf}
}

@article{pisingerQuadraticKnapsackProblem2007,
  title = {The Quadratic Knapsack Problem\textemdash a Survey},
  author = {Pisinger, David},
  year = {2007},
  month = mar,
  journal = {Discrete Applied Mathematics},
  volume = {155},
  number = {5},
  pages = {623--648},
  issn = {0166218X},
  doi = {10.1016/j.dam.2006.08.007},
  urldate = {2022-10-26},
  abstract = {The binary quadratic knapsack problem maximizes a quadratic objective function subject to a linear capacity constraint. Due to its simple structure and challenging difficulty it has been studied intensively during the last two decades. The present paper gives a survey of upper bounds presented in the literature, and show the relative tightness of several of the bounds. Techniques for deriving the bounds include relaxation from upper planes, linearization, reformulation, Lagrangian relaxation, Lagrangian decomposition, and semidefinite programming. A short overview of heuristics, reduction techniques, branch-and-bound algorithms and approximation results is given, followed by an overview of valid inequalities for the quadratic knapsack polytope. The paper is concluded by an experimental study where the upper bounds presented are compared with respect to strength and computational effort.},
  langid = {english},
  file = {C\:\\Users\\irene\\Zotero\\storage\\RJ82WUKI\\Pisinger - 2007 - The quadratic knapsack problem—a survey.pdf}
}

@article{poyatosEvoPruneDeepTLEvolutionaryPruning2023a,
  title = {{{EvoPruneDeepTL}}: {{An}} Evolutionary Pruning Model for Transfer Learning Based Deep Neural Networks},
  shorttitle = {{{EvoPruneDeepTL}}},
  author = {Poyatos, Javier and Molina, Daniel and Martinez, Aritz D. and Del Ser, Javier and Herrera, Francisco},
  year = {2023},
  month = jan,
  journal = {Neural Networks},
  volume = {158},
  pages = {59--82},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2022.10.011},
  urldate = {2023-05-31},
  abstract = {In recent years, Deep Learning models have shown a great performance in complex optimization problems. They generally require large training datasets, which is a limitation in most practical cases. Transfer learning allows importing the first layers of a pre-trained architecture and connecting them to fully-connected layers to adapt them to a new problem. Consequently, the configuration of the these layers becomes crucial for the performance of the model. Unfortunately, the optimization of these models is usually a computationally demanding task. One strategy to optimize Deep Learning models is the pruning scheme. Pruning methods are focused on reducing the complexity of the network, assuming an expected performance penalty of the model once pruned. However, the pruning could potentially be used to improve the performance, using an optimization algorithm to identify and eventually remove unnecessary connections among neurons. This work proposes EvoPruneDeepTL, an evolutionary pruning model for Transfer Learning based Deep Neural Networks which replaces the last fully-connected layers with sparse layers optimized by a genetic algorithm. Depending on its solution encoding strategy, our proposed model can either perform optimized pruning or feature selection over the densely connected part of the neural network. We carry out different experiments with several datasets to assess the benefits of our proposal. Results show the contribution of EvoPruneDeepTL and feature selection to the overall computational efficiency of the network as a result of the optimization process. In particular, the accuracy is improved, reducing at the same time the number of active neurons in the final layers.},
  langid = {english},
  keywords = {Deep learning,Evolutionary algorithms,Feature selection,Pruning,Transfer learning},
  note = {1 citations (Crossref) [2023-05-31]}
}

@article{reevesFeatureArticleGenetic1997,
  title = {Feature {{Article}}\textemdash{{Genetic Algorithms}} for the {{Operations Researcher}}},
  author = {Reeves, Colin R.},
  year = {1997},
  month = aug,
  journal = {INFORMS Journal on Computing},
  volume = {9},
  number = {3},
  pages = {231--250},
  publisher = {{INFORMS}},
  issn = {1091-9856},
  doi = {10.1287/ijoc.9.3.231},
  urldate = {2023-05-31},
  abstract = {Genetic algorithms have become increasingly popular as a means of solving hard combinatorial optimization problems of the type familiar in operations research. This feature article will consider what genetic algorithms have achieved in this area, discuss some of the factors that influence their success or failure, and offer a guide for operations researchers who want to get the best out of them.},
  keywords = {genetic algorithms,heuristics,optimization},
  file = {C\:\\Users\\irene\\Zotero\\storage\\3Y3MFBZH\\Reeves - 1997 - Feature Article—Genetic Algorithms for the Operati.pdf}
}

@incollection{reevesGeneticAlgorithms2010,
  title = {Genetic {{Algorithms}}},
  booktitle = {Handbook of {{Metaheuristics}}},
  author = {Reeves, Colin},
  year = {2010},
  month = sep,
  volume = {146},
  pages = {109--139},
  doi = {10.1007/978-1-4419-1665-5_5},
  abstract = {Genetic algorithms (GAs) have become popular as a means of solving hard combinatorial optimization problems. The first part of this chapter briefly traces their history, explains the basic concepts and discusses some of their theoretical aspects. It also references a number of sources for further research into their applications. The second part concentrates on the detailed implementation of a GA. It discusses the fundamentals of encoding a `genotype' in different circumstances and describes the mechanics of population selection and management and the choice of genetic `operators' for generating new populations. In closing, some specific guidelines for using GAs in practice are provided.},
  file = {C\:\\Users\\irene\\Zotero\\storage\\NJII4H5F\\Reeves - 2010 - Genetic Algorithms.pdf}
}

@article{rodriguezALGORITMOSBIOINSPIRADOSCON,
  title = {{ALGORITMOS BIOINSPIRADOS CON INTERACCIONES NEGATIVAS}},
  author = {Rodr{\'i}guez, Carmen Biedma and Cabrera, Daniel Molina and Triguero, Francisco Herrera},
  abstract = {We know that all metaheuristics have a very important aspect to consider if we want good results, the diversity. There are a lot of techniques to provide this quality, but a few years ago appeared once called repulsion. With this component we will achieve that our set of solutions keep the diversity every moment, consequenquentially we will have better solutions and the avoidance of local minimums.},
  langid = {spanish},
  file = {C\:\\Users\\irene\\Zotero\\storage\\7HW4EBBY\\Rodríguez et al. - ALGORITMOS BIOINSPIRADOS CON INTERACCIONES NEGATIV.pdf}
}

@article{rojasMemeticMicrogeneticAlgorithms2023,
  title = {Memetic Micro-Genetic Algorithms for Cancer Data Classification},
  author = {Rojas, Mat{\'i}as Gabriel and Olivera, Ana Carolina and Carballido, Jessica Andrea and Vidal, Pablo Javier},
  year = {2023},
  month = feb,
  journal = {Intelligent Systems with Applications},
  volume = {17},
  pages = {200173},
  issn = {26673053},
  doi = {10.1016/j.iswa.2022.200173},
  urldate = {2023-03-15},
  langid = {english},
  file = {C\:\\Users\\irene\\Zotero\\storage\\W7EAEFRD\\Rojas et al. - 2023 - Memetic micro-genetic algorithms for cancer data c.pdf}
}

@incollection{simoesCHCBasedAlgorithmsDynamic2011,
  title = {{{CHC-Based Algorithms}} for the {{Dynamic Traveling Salesman Problem}}},
  booktitle = {Applications of {{Evolutionary Computation}}},
  author = {Sim{\~o}es, Anabela and Costa, Ernesto},
  editor = {Di Chio, Cecilia and Cagnoni, Stefano and Cotta, Carlos and Ebner, Marc and Ek{\'a}rt, Anik{\'o} and {Esparcia-Alc{\'a}zar}, Anna I. and Merelo, Juan J. and Neri, Ferrante and Preuss, Mike and Richter, Hendrik and Togelius, Julian and Yannakakis, Georgios N.},
  year = {2011},
  volume = {6624},
  pages = {354--363},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-20525-5_36},
  urldate = {2023-05-13},
  abstract = {The CHC algorithm uses an elitist selection method that, combined with an incest prevention mechanism and a method to diverge the population whenever it converges, allows the maintenance of the population diversity. This algorithm was successfully used in the past for static optimization problems. The use of memory in Evolutionary Algorithms has been proved to be advantageous when dealing with dynamic optimization problems. In this paper we investigate the use of three different explicit memory strategies included in the CHC algorithm. These strategies - direct, immigrant and associative - combined with the CHC algorithm are used to solve different instances of the dynamic Traveling Salesman Problem in cyclic, noisy and random environments. The experimental results, statistically validated, show that the memory schemes significantly improve the performance of the original CHC algorithm for all types of studied environments. Moreover, when compared with the equivalent memory-based standard EAs with the same memory schemes, the memory-based CHC algorithms obtain superior results when the environmental changes are slower.},
  isbn = {978-3-642-20524-8 978-3-642-20525-5},
  langid = {english},
  file = {C\:\\Users\\irene\\Zotero\\storage\\CWVGL5EC\\Simões y Costa - 2011 - CHC-Based Algorithms for the Dynamic Traveling Sal.pdf}
}

@article{smithOperatorParameterAdaptation1997,
  title = {Operator and Parameter Adaptation in Genetic Algorithms},
  author = {Smith, Jim and Fogarty, T. C.},
  year = {1997},
  month = jan,
  journal = {Soft Computing},
  volume = {1},
  number = {2},
  publisher = {{Springer (part of Springer Nature)}},
  issn = {1432-7643},
  doi = {10.1007/s005000050009},
  urldate = {2023-05-31},
  abstract = {Genetic Algorithms are a class of powerful, robust search techniques based on genetic inheritance and the Darwinian metaphor of ``Natural Selection''. These algorithms maintain a finite memory of individual points on the search landscape known as the ``population''. Members of the population are usually represented as strings written over some fixed alphabet, each of which has a scalar value attached to it reflecting its quality or ``fitness''. The search may be seen as the iterative application of a number of operators, such as selection, recombination and mutation, to the population with the aim of producing progressively fitter individuals. These operators are usually static, that is to say that their mechanisms, parameters, and probability of application are fixed at the beginning and constant throughout the run of the algorithm. However there is an increasing body of evidence that not only is there no single choice of operators which is optimal for all problems, but that in fact the optimal choice of operators for a given problem will be time-variant i.e. it will depend on such factors as the degree of convergence of the population. Based on theoretical and practical approaches, a number of authors have proposed methods of adaptively controlling one or more of the operators, usually invoking some kind of ``meta-learning'' algorithm, in order to try and improve the performance of the Genetic Algorithm as a function optimiser. In this paper we describe the background to these approaches, and suggest a framework for their classification based on the learning strategy used to control them, and what facets of the algorithm are susceptible to adaptation. We then review a number of significant pieces of work within this context, and draw some conclusions about the relative merits of various approaches and promising directions for future work.},
  langid = {english},
  file = {C\:\\Users\\irene\\Zotero\\storage\\N3KCIVP9\\Smith y Fogarty - 1997 - Operator and parameter adaptation in genetic algor.pdf}
}

@article{tuRobustStochasticGenetic2004,
  title = {A {{Robust Stochastic Genetic Algorithm}} ({{StGA}}) for {{Global Numerical Optimization}}},
  author = {Tu, Z. and Lu, Y.},
  year = {2004},
  month = oct,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {8},
  number = {5},
  pages = {456--470},
  issn = {1089-778X},
  doi = {10.1109/TEVC.2004.831258},
  urldate = {2023-05-31},
  abstract = {Many real-life problems can be formulated as numerical optimization of certain objective functions. However, often an objective function possesses numerous local optima, which could trap an algorithm from moving toward the desired global solution. Evolutionary algorithms (EAs) have emerged to enable global optimization; however, at the present stage, EAs are basically limited to solving small-scale problems due to the constraint of computational efficiency. To improve the search efficiency, this paper presents a stochastic genetic algorithm (StGA). A novel stochastic coding strategy is employed so that the search space is dynamically divided into regions using a stochastic method and explored region-by-region. In each region, a number of children are produced through random sampling, and the best child is chosen to represent the region. The variance values are decreased if at least one of five generated children results in improved fitness, otherwise, the variance values are increased. Experiments on 20 test functions of diverse complexities show that the StGA is able to find the near-optimal solution in all cases. Compared with several other algorithms, StGA achieves not only an improved accuracy, but also a considerable reduction of the computational effort. On average, the computational cost required by StGA is about one order less than the other algorithms. The StGA is also shown to be able to solve large-scale problems.},
  langid = {english},
  file = {C\:\\Users\\irene\\Zotero\\storage\\ZN4WS3BR\\Tu y Lu - 2004 - A Robust Stochastic Genetic Algorithm (StGA) for G.pdf}
}

@article{vargasEstrategiasParaMejorar,
  title = {{Estrategias para mejorar el Balance entre Exploraci\'on y Explotaci\'on en Optimizaci\'on de Enjambre de Part\'iculas}},
  author = {Vargas, Lic Yenny Noa},
  langid = {spanish},
  note = {Mirar referencias, principalmente para introduccion},
  file = {C\:\\Users\\irene\\Zotero\\storage\\3J2C2GWK\\Vargas - Estrategias para mejorar el Balance entre Explorac.pdf}
}

@article{wangHybridMetaheuristicAlgorithm2021,
  title = {Hybrid Metaheuristic Algorithm Using Butterfly and Flower Pollination Base on Mutualism Mechanism for Global Optimization Problems},
  author = {Wang, Zhongmin and Luo, Qifang and Zhou, Yongquan},
  year = {2021},
  month = oct,
  journal = {Engineering with Computers},
  volume = {37},
  number = {4},
  pages = {3665--3698},
  issn = {0177-0667, 1435-5663},
  doi = {10.1007/s00366-020-01025-8},
  urldate = {2023-02-18},
  abstract = {The butterfly optimization algorithm (BOA) is a new metaheuristic algorithm that is inspired from food foraging behavior of the butterflies. Because of its simplicity and effectiveness, the algorithm has been proved to be effective in solving global optimization problems and applied to practical problems. However, BOA is prone to local optimality and may lose its diversity, thus suffering losses of premature convergence. In this work, a hybrid metaheuristic algorithm using butterfly and flower pollination base on mutualism mechanism called MBFPA was proposed. Firstly, the flower pollination algorithm has good exploration ability and the hybrid butterfly optimization algorithm and the flower pollination algorithms greatly improve the exploration ability of the algorithm; secondly, the symbiosis organisms search has a strong exploitation capability in the mutualism phase. By introducing the mutualism phase, the algorithm's exploitation capability is effectively increased and the algorithm's convergence speed is accelerated. Finally, the adaptive switching probability is increased to increase the algorithm's balance in exploration and exploitation capabilities. In order to evaluate the effectiveness of the algorithm, in the 49 standard test functions, the proposed algorithm was compared with six basic metaheuristic algorithms and five hybrid metaheuristic algorithms. MBFPA has also been used to solve five classic engineering problems (three-bar truss design problem; multi-plate disc clutch brake design; welded beam design; pressure vessel design problem; and speed reducer design). The results show that the proposed method is feasible and has good application prospect and competitiveness.},
  langid = {english},
  file = {C\:\\Users\\irene\\Zotero\\storage\\IENPY2WT\\Wang et al. - 2021 - Hybrid metaheuristic algorithm using butterfly and.pdf}
}

@article{witzgallMathematicalMethodsSite1975,
  title = {Mathematical Methods of Site Selection for {{Electronic Message Systems}} ({{EMS}})},
  author = {Witzgall, C.},
  year = {1975},
  month = jun,
  journal = {NASA STI/Recon Technical Report N},
  volume = {76},
  pages = {18321},
  urldate = {2023-06-02},
  abstract = {The concept of electronic message (mail) transmission was the subject of several feasibility studies during the past decade. It requires the installation of electronic message handling facilities at selected locations. If transmission is to be via communications satellite, then any such facility can transmit to and receive from any other one. In the report, the mathematical aspects of choosing the number and locations of these facilities are examined. An inventory of solution methods is presented, along with recommendations as to which among them should be employed or developed further.},
  keywords = {Communications and Radar,Cost Analysis,Linear Programming,Messages,Network Synthesis,Radio Relay Systems,Systems Engineering,Telecommunication},
  annotation = {ADS Bibcode: 1975STIN...7618321W}
}

@misc{wongEvolutionaryMultimodalOptimization2015,
  title = {Evolutionary {{Multimodal Optimization}}: {{A Short Survey}}},
  shorttitle = {Evolutionary {{Multimodal Optimization}}},
  author = {Wong, Ka-Chun},
  year = {2015},
  month = aug,
  number = {arXiv:1508.00457},
  eprint = {1508.00457},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  urldate = {2023-03-12},
  abstract = {Real world problems always have different multiple solutions. For instance, optical engineers need to tune the recording parameters to get as many optimal solutions as possible for multiple trials in the varied-line-spacing holographic grating design problem. Unfortunately, most traditional optimization techniques focus on solving for a single optimal solution. They need to be applied several times; yet all solutions are not guaranteed to be found. Thus the multimodal optimization problem was proposed. In that problem, we are interested in not only a single optimal point, but also the others. With strong parallel search capability, evolutionary algorithms are shown to be particularly effective in solving this type of problem. In particular, the evolutionary algorithms for multimodal optimization usually not only locate multiple optima in a single run, but also preserve their population diversity throughout a run, resulting in their global optimization ability on multimodal functions. In addition, the techniques for multimodal optimization are borrowed as diversity maintenance techniques to other problems. In this chapter, we describe and review the state-of-the-arts evolutionary algorithms for multimodal optimization in terms of methodology, benchmarking, and application.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Quantitative Methods},
  file = {C\:\\Users\\irene\\Zotero\\storage\\46MBKNGY\\Wong - 2015 - Evolutionary Multimodal Optimization A Short Surv.pdf;C\:\\Users\\irene\\Zotero\\storage\\GLGC39XH\\1508.html}
}
